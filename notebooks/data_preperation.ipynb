{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7934d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to ./punkt...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import requests\n",
    "import pymupdf\n",
    "import io \n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt_tab', download_dir='./punkt')  # Ensure that NLTK's sentence tokenizer is available\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.data.path.append('./punkt')\n",
    "import psycopg2\n",
    "import hashlib\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d984a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config Start ---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "url = \"https://www.who.int/europe/publications/i\"\n",
    "\n",
    "load = \"yes\" # change to no, if you don't want to load data \n",
    "pages = 1\n",
    "load_dotenv()\n",
    "\n",
    "# api_key = os.getenv('OPENAI_API_KEY')\n",
    "# client = OpenAI(api_key=HF_TOKEN)\n",
    "\n",
    "# Use HF_TOKEN instead of OpenAI key\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "if not HF_TOKEN:\n",
    "    raise RuntimeError(\"HF_TOKEN not set in environment. Add HF_TOKEN=your_token to your .env\")\n",
    "\n",
    "# Choose a sentence-embedding model available on HF. Example: all-mpnet-base-v2\n",
    "HF_EMBEDDING_MODEL = os.getenv(\"HF_EMBEDDING_MODEL\", \"sentence-transformers/all-mpnet-base-v2\")\n",
    "HF_API_BASE = \"https://api-inference.huggingface.co/pipeline/feature-extraction\"\n",
    "HF_API_URL = f\"{HF_API_BASE}/{HF_EMBEDDING_MODEL}\"\n",
    "\n",
    "# Session for requests (re-uses connections, sets headers)\n",
    "requests_session = requests.Session()\n",
    "requests_session.headers.update({\n",
    "    \"Authorization\": f\"Bearer {HF_TOKEN}\",\n",
    "    \"User-Agent\": \"who-rag-bot/0.1 (+contact@example.com)\"\n",
    "})\n",
    "\n",
    "# Config End ----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9bed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure punkt tokenizer is present (download once)\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c176a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_driver():\n",
    "    opts = Options()\n",
    "    opts.add_argument(\"--headless\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    # ChromeDriverManager automatically downloads/sets up ChromeDriver\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(), options=opts)\n",
    "    return driver\n",
    "\n",
    "\n",
    "# def make_driver():\n",
    "#     opts = Options()\n",
    "#     opts.add_argument(\"--headless=new\")\n",
    "#     opts.add_argument(\"--no-sandbox\")\n",
    "#     opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "#     # add any other opts you need (download dir if you plan to actually download files etc.)\n",
    "#     driver = webdriver.Chrome(ChromeDriverManager().install(), options=opts)\n",
    "#     return driver\n",
    "\n",
    "\n",
    "def fetch_links(start: int, end: int, url: str, pause: int) -> set:\n",
    "    \"\"\"\n",
    "    Fetch pdf links from paginated WHO listing page (simple approach).\n",
    "    \"\"\"\n",
    "    driver = make_driver()\n",
    "    driver.get(url)\n",
    "    download_links = set()\n",
    "\n",
    "    for page_number in range(start, end + 1):\n",
    "        try:\n",
    "            input_field = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'input.k-textbox'))\n",
    "            )\n",
    "\n",
    "            input_field.clear()\n",
    "            input_field.send_keys(str(page_number))\n",
    "\n",
    "            next_page_button = driver.find_element(By.CSS_SELECTOR, 'a[aria-label=\"Go to the next page\"]')\n",
    "            next_page_button.click()\n",
    "\n",
    "            time.sleep(pause)\n",
    "\n",
    "            links = driver.find_elements(By.XPATH, \"//a[contains(@href, '.pdf')]\")\n",
    "            for link in links:\n",
    "                href = link.get_attribute(\"href\")\n",
    "                if href:\n",
    "                    download_links.add(href)\n",
    "            print(f\"Loaded page {page_number}... {len(download_links)} in set.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred on page {page_number}: {e}\")\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return download_links\n",
    "\n",
    "\n",
    "def fetch_pdf(link: str) -> str:\n",
    "    \"\"\"\n",
    "    Download PDF via requests and extract text using PyMuPDF (fitz).\n",
    "    \"\"\"\n",
    "    resp = requests_session.get(link, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    filestream = io.BytesIO(resp.content)\n",
    "    with fitz.open(stream=filestream, filetype=\"pdf\") as doc:\n",
    "        text = []\n",
    "        for page in doc:\n",
    "            text.append(page.get_text(\"text\"))\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "\n",
    "def split_large_paragraph(paragraph: str, max_words: int, overlap: int) -> list:\n",
    "    \"\"\"\n",
    "    Break a large paragraph into word-windowed chunks using sentence tokenization for better boundaries.\n",
    "    Returns list[str].\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words_in_sentence = sentence.split()\n",
    "        sentence_len = len(words_in_sentence)\n",
    "\n",
    "        if current_word_count + sentence_len > max_words and current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            # keep overlap words (from end)\n",
    "            if overlap > 0:\n",
    "                tail = \" \".join(current_chunk).split()[-overlap:]\n",
    "                current_chunk = tail.copy()\n",
    "                current_word_count = len(current_chunk)\n",
    "            else:\n",
    "                current_chunk = []\n",
    "                current_word_count = 0\n",
    "\n",
    "        current_chunk.extend(words_in_sentence)\n",
    "        current_word_count += sentence_len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def split_into_chunks(text: str, max_words=512, overlap=50) -> list:\n",
    "    \"\"\"\n",
    "    Heuristic splitter that recognizes headings/bullets and then uses sentence/window chunking.\n",
    "    Returns list of dicts: {section, type, content}\n",
    "    \"\"\"\n",
    "    heading_pattern = r'^(?P<heading>[A-Z].+):$'\n",
    "    bullet_point_pattern = r'^\\s*[-•]\\s*(?P<bullet>.+)'\n",
    "\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_section = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        heading_match = re.match(heading_pattern, line)\n",
    "        if heading_match:\n",
    "            if current_chunk:\n",
    "                paragraph = ' '.join(current_chunk)\n",
    "                for sub_chunk in split_large_paragraph(paragraph, max_words, overlap):\n",
    "                    chunks.append({'section': current_section, 'type': 'paragraph', 'content': sub_chunk})\n",
    "                current_chunk = []\n",
    "\n",
    "            current_section = heading_match.group('heading')\n",
    "            chunks.append({'section': current_section, 'type': 'heading', 'content': line})\n",
    "            continue\n",
    "\n",
    "        bullet_match = re.match(bullet_point_pattern, line)\n",
    "        if bullet_match:\n",
    "            if current_chunk:\n",
    "                paragraph = ' '.join(current_chunk)\n",
    "                for sub_chunk in split_large_paragraph(paragraph, max_words, overlap):\n",
    "                    chunks.append({'section': current_section, 'type': 'paragraph', 'content': sub_chunk})\n",
    "                current_chunk = []\n",
    "\n",
    "            chunks.append({'section': current_section, 'type': 'bullet', 'content': bullet_match.group('bullet')})\n",
    "            continue\n",
    "\n",
    "        current_chunk.append(line)\n",
    "\n",
    "    if current_chunk:\n",
    "        paragraph = ' '.join(current_chunk)\n",
    "        for sub_chunk in split_large_paragraph(paragraph, max_words, overlap):\n",
    "            chunks.append({'section': current_section, 'type': 'paragraph', 'content': sub_chunk})\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def create_vector(texts: list, batch_size: int = 8) -> list:\n",
    "    \"\"\"\n",
    "    Use Hugging Face Inference API (feature-extraction pipeline) to obtain embeddings.\n",
    "    - texts: list[str]\n",
    "    - returns: list[list[float]] (one embedding per input)\n",
    "    NOTE: model dimension depends on HF_EMBEDDING_MODEL (update DB/vector store accordingly)\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    if not texts:\n",
    "        return []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        # HF pipeline accepts either a single input or list of inputs\n",
    "        resp = requests_session.post(HF_API_URL, json={\"inputs\": batch}, timeout=120)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        # data should be a list of embedding lists (one per input)\n",
    "        # Some HF endpoints might return a single embedding if input is single; normalize:\n",
    "        if isinstance(data, list) and isinstance(data[0], list):\n",
    "            embeddings.extend(data)\n",
    "        else:\n",
    "            # fallback: treat as one embedding per input\n",
    "            embeddings.append(data)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def hash_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    SHA256 hex digest for a single string.\n",
    "    \"\"\"\n",
    "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def hash_texts(texts: list) -> list:\n",
    "    return [hash_text(t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db3af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage (pseudo):\n",
    "# if __name__ == \"__main__\":\n",
    "#     links = fetch_links(1, PAGES, URL, pause=2)\n",
    "#     for link in links:\n",
    "#         text = fetch_pdf(link)\n",
    "#         chunks = split_into_chunks(text, max_words=512, overlap=50)\n",
    "#         texts = [c[\"content\"] for c in chunks if c[\"type\"] != \"heading\"]\n",
    "#         emb = create_vector(texts)\n",
    "#         hashes = hash_texts(texts)\n",
    "#         print(\"Got\", len(emb), \"embeddings and\", len(hashes), \"hashes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4deecd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "hf_client = InferenceClient(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e20d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_vector(chunks: list) -> list:\n",
    "#     \"\"\"\n",
    "#     Create vectors from text using Hugging Face embeddings.\n",
    "#     \"\"\"\n",
    "#     # Example model: sentence-transformers/all-MiniLM-L6-v2\n",
    "#     # This produces 384-dimensional embeddings (so adjust DB schema!)\n",
    "#     response = hf_client.feature_extraction(\n",
    "#         model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "#         inputs=chunks\n",
    "#     )\n",
    "    \n",
    "#     # Response will be a list of embeddings for each input\n",
    "#     if isinstance(response[0], list):  \n",
    "#         return response[0]  # return the first vector if single text\n",
    "#     return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee88d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_to_database(chunks: list, group_id: int) -> None:\n",
    "    \"\"\"\n",
    "    Connect to Postgres, ensure table exists,\n",
    "    batch-embed chunks and ingest them.\n",
    "    \"\"\"\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"postgres\",\n",
    "        port=5432,\n",
    "        database=\"llm\",\n",
    "        user=\"admin\",\n",
    "        password=\"admin\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Adjust VECTOR dimension to match your HF model (MiniLM = 384)\n",
    "    cur.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS text_chunks (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        section_content TEXT,\n",
    "        type TEXT,\n",
    "        vector VECTOR(384),  -- match embedding dimension\n",
    "        hash TEXT UNIQUE,\n",
    "        group_id INT,\n",
    "        timestamp TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "    # Prepare texts for embedding\n",
    "    texts = [(chunk['section'] or \"\") + \" \" + chunk['content'] for chunk in chunks]\n",
    "    vectors = create_vectors(texts)\n",
    "\n",
    "    # Insert all chunks\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        section_content = texts[i]\n",
    "        chunk_type = chunk['type']\n",
    "        vector = vectors[i]\n",
    "        hash_value = hash_text(section_content)\n",
    "\n",
    "        cur.execute('''\n",
    "            INSERT INTO text_chunks (section_content, type, vector, hash, group_id)\n",
    "            VALUES (%s, %s, %s, %s, %s)\n",
    "            ON CONFLICT (hash) DO NOTHING;\n",
    "        ''', (section_content, chunk_type, vector, hash_value, group_id))\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "762e32da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChromeDriverManager' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load == \u001b[33m\"\u001b[39m\u001b[33myes\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     links = \u001b[43mfetch_links\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mpages\u001b[49m\u001b[43m,\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     number = \u001b[32m0\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m links:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mfetch_links\u001b[39m\u001b[34m(start, end, url, pause)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch_links\u001b[39m(start: \u001b[38;5;28mint\u001b[39m, end: \u001b[38;5;28mint\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m, pause: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mset\u001b[39m:\n\u001b[32m     23\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m    Fetch pdf links from paginated WHO listing page (simple approach).\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     driver = \u001b[43mmake_driver\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     driver.get(url)\n\u001b[32m     28\u001b[39m     download_links = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mmake_driver\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m opts.add_argument(\u001b[33m\"\u001b[39m\u001b[33m--disable-dev-shm-usage\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# ChromeDriverManager automatically downloads/sets up ChromeDriver\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m driver = webdriver.Chrome(\u001b[43mChromeDriverManager\u001b[49m().install(), options=opts)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m driver\n",
      "\u001b[31mNameError\u001b[39m: name 'ChromeDriverManager' is not defined"
     ]
    }
   ],
   "source": [
    "if load == \"yes\":\n",
    "    links = fetch_links(1,pages,url,5)\n",
    "    number = 0\n",
    "    for link in links:\n",
    "        fetch_file = fetch_pdf(link)\n",
    "        number += 1\n",
    "        result = split_into_chunks(fetch_file)\n",
    "        ingest_to_database(result, number)\n",
    "        print(\"Data ingested successfully! Loaded link number \" + str(number))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32d98cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_embedding(query: str) -> list:\n",
    "    \"\"\"\n",
    "    Generate embedding for the user's query (HF version).\n",
    "    \"\"\"\n",
    "    response = hf_client.feature_extraction(\n",
    "        model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        inputs=[query]\n",
    "    )\n",
    "    return response[0]  # single vector\n",
    "\n",
    "\n",
    "def search_similar_chunks_pg(query_vector, top_n=5):\n",
    "    \"\"\"\n",
    "    Search using pgvector cosine similarity directly in SQL.\n",
    "    \"\"\"\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"postgres\",\n",
    "        port=5432,\n",
    "        database=\"llm\",\n",
    "        user=\"admin\",\n",
    "        password=\"admin\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Perform similarity search directly in SQL\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        SELECT id, section_content, group_id, 1 - (vector <=> %s::vector) AS similarity\n",
    "        FROM text_chunks\n",
    "        ORDER BY vector <=> %s::vector\n",
    "        LIMIT %s;\n",
    "        \"\"\",\n",
    "        (query_vector, query_vector, top_n)\n",
    "    )\n",
    "\n",
    "    rows = cur.fetchall()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    return rows  # (id, content, group_id, similarity)\n",
    "\n",
    "\n",
    "def search_similar_chunks_faiss(query_vector, top_n=5):\n",
    "    \"\"\"\n",
    "    Alternative FAISS-based retrieval (if you want to load all vectors).\n",
    "    Not needed if pgvector is available.\n",
    "    \"\"\"\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"postgres\",\n",
    "        port=5432,\n",
    "        database=\"llm\",\n",
    "        user=\"admin\",\n",
    "        password=\"admin\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('SELECT id, section_content, vector, group_id FROM text_chunks')\n",
    "    rows = cur.fetchall()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    # Build FAISS index\n",
    "    import faiss\n",
    "    d = len(query_vector)\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "\n",
    "    vectors, ids, contents, groups = [], [], [], []\n",
    "    for row in rows:\n",
    "        chunk_id, section_content, vector, group_id = row\n",
    "        chunk_vector = np.array(vector, dtype=np.float32)\n",
    "        vectors.append(chunk_vector)\n",
    "        ids.append(chunk_id)\n",
    "        contents.append(section_content)\n",
    "        groups.append(group_id)\n",
    "\n",
    "    vectors_np = np.vstack(vectors).astype(np.float32)\n",
    "    index.add(vectors_np)\n",
    "\n",
    "    query_np = np.array(query_vector, dtype=np.float32).reshape(1, -1)\n",
    "    distances, indices = index.search(query_np, top_n)\n",
    "\n",
    "    return [(ids[i], contents[i], float(distances[0][n]), groups[i]) for n, i in enumerate(indices[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27bf9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_chunks_faiss_dot(query_vector, top_n=5):\n",
    "    \"\"\"Cosine similarity search using FAISS (via normalized dot-product).\"\"\"\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"postgres\",\n",
    "        port=5432,\n",
    "        database=\"llm\",\n",
    "        user=\"admin\",\n",
    "        password=\"admin\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('SELECT id, section_content, vector, group_id FROM text_chunks')\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    vectors = []\n",
    "    ids = []\n",
    "    contents = []\n",
    "    groups = []\n",
    "\n",
    "    for row in rows:\n",
    "        chunk_id, section_content, vector_str, group_id = row\n",
    "        chunk_vector = parse_pgvector(vector_str)\n",
    "        chunk_vector = normalize_vector(chunk_vector)\n",
    "        vectors.append(chunk_vector)\n",
    "        ids.append(chunk_id)\n",
    "        contents.append(section_content)\n",
    "        groups.append(group_id)\n",
    "\n",
    "    # Build FAISS index\n",
    "    d = len(query_vector)\n",
    "    index = faiss.IndexFlatIP(d)  # Inner product index\n",
    "    vectors_np = np.vstack(vectors).astype(np.float32)\n",
    "    index.add(vectors_np)\n",
    "\n",
    "    # Normalize query vector\n",
    "    query_vector_np = normalize_vector(np.array(query_vector, dtype=np.float32)).reshape(1, -1)\n",
    "\n",
    "    # Search\n",
    "    similarities, indices = index.search(query_vector_np, top_n)\n",
    "\n",
    "    faiss_dot_results = [\n",
    "        (ids[i], contents[i], float(similarities[0][n]), groups[i])\n",
    "        for n, i in enumerate(indices[0])\n",
    "    ]\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    return faiss_dot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ac7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_chunks_canberra(query_vector, top_n=5):\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"postgres\",\n",
    "        port=5432,\n",
    "        database=\"llm\",\n",
    "        user=\"admin\",\n",
    "        password=\"admin\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('SELECT id, section_content, vector, group_id FROM text_chunks')\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    similarities = []\n",
    "    for row in rows:\n",
    "        chunk_id, section_content, vector_str, group_id = row\n",
    "        chunk_vector = parse_pgvector(vector_str)  # use the robust parser\n",
    "        distance = canberra_distance(query_vector, chunk_vector)\n",
    "        similarities.append((chunk_id, section_content, distance, group_id))\n",
    "\n",
    "    similarities = sorted(similarities, key=lambda x: x[2])[:top_n]\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f34bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(results, method_name):\n",
    "    print(f\"\\nTop Results ({method_name}):\")\n",
    "    for i, (chunk_id, section_content, similarity, group_id) in enumerate(results, 1):\n",
    "        snippet = section_content[:300] + (\"...\" if len(section_content) > 300 else \"\")\n",
    "        print(f\"{i}. ID: {chunk_id} | Similarity: {similarity:.4f} | Group_id: {group_id}\\nText: {snippet}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e7194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_top_ids_simple(pg_results, faiss_l2_results, faiss_dot_results, canberra_distance_results, top_n=3):\n",
    "    id_scores = defaultdict(int)\n",
    "    id_group_mapping = {}\n",
    "\n",
    "    def assign_scores(results):\n",
    "        for rank, result in enumerate(results[:top_n], start=1):\n",
    "            chunk_id = result[0]\n",
    "            group_id = result[3]\n",
    "            score = top_n - rank + 1\n",
    "            id_scores[chunk_id] += score\n",
    "            id_group_mapping[chunk_id] = group_id\n",
    "\n",
    "    for results in [pg_results, faiss_l2_results, faiss_dot_results, canberra_distance_results]:\n",
    "        assign_scores(results)\n",
    "\n",
    "    sorted_ids = sorted(id_scores.items(), key=lambda x: -x[1])\n",
    "    final_output = [(chunk_id, id_group_mapping[chunk_id], score) for chunk_id, score in sorted_ids[:top_n]]\n",
    "    return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f15047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_chunk_by_id_from_db(chunk_id):\n",
    "#     \"\"\"Fetch the chunk text directly from the database using the best chunk ID.\"\"\"\n",
    "#     # Connect to PostgreSQL\n",
    "#     conn = psycopg2.connect(\n",
    "#         host=\"postgres\",\n",
    "#         port=5432,\n",
    "#         database=\"llm\",\n",
    "#         user=\"admin\",\n",
    "#         password=\"admin\"\n",
    "#     )\n",
    "#     cur = conn.cursor()\n",
    "\n",
    "#     # Query the database for the chunk by its ID\n",
    "#     cur.execute(\"SELECT section_content FROM text_chunks WHERE id = %s\", (chunk_id,))\n",
    "#     result = cur.fetchone()\n",
    "\n",
    "#     cur.close()\n",
    "#     conn.close()\n",
    "\n",
    "#     if result:\n",
    "#         return result[0]  # Return the text content of the chunk\n",
    "#     else:\n",
    "#         return None  # In case the chunk ID is not found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c814531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def get_chunk_by_id_from_db(chunk_id: int) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Fetch the chunk text directly from the database using the given chunk ID.\n",
    "    \n",
    "    Returns:\n",
    "        The chunk's text content, or None if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with psycopg2.connect(\n",
    "            host=\"postgres\",\n",
    "            port=5432,\n",
    "            database=\"llm\",\n",
    "            user=\"admin\",\n",
    "            password=\"admin\"\n",
    "        ) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(\n",
    "                    sql.SQL(\"SELECT section_content FROM text_chunks WHERE id = %s\"),\n",
    "                    (chunk_id,)\n",
    "                )\n",
    "                result = cur.fetchone()\n",
    "\n",
    "        if result:\n",
    "            return result[0]\n",
    "        return None\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database error while fetching chunk {chunk_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29815e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d074fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
